the idea is that you do not need a neural network where each neuron in one layer is connected to each neuron in the previous layer.
- it would be better to account for ***spatial structure*** of the images in the learning process
~~~
they use 3 main ideas;

1 -> local receptive fields
2 -> shared weights
3 -> pooling
~~~

### local receptive fields
`the idea that each neuron in the first layer only connects to a localized region of neurons from the input layer`
- this may be a 3x3 region, for example
- we keep sliding this region across the input layer, so that there is a unique hidden layer neuron for each and every 3x3 region
- additionally, the hidden layer neuron will have a bias, along with one set of weights

### shared weights and biases
`ALL of the neurons in the first layer are to share the same exact weights and bias`
- meaning, they look for the exact same feature in the image
- this sort of "mapping" creates a "`feature map`", where the feature being looked for is mapped to a single hidden layer

in the hidden layers, the $(j,k)^{th}$ hidden neuron computes:
$$
\displaylines
{
\sigma(\color{green}\sum_{l=0}^{2}\sum_{m=0}^{2}w_{l,m}\color{red}a_{j+l,k+m}\color{white})
\\
\color{green}\to \color{white}\text{self explanatory, go thru weight matrix row–by–row}
\\
\color{red}\to \color{white}\text{think of j and k as "starting points" for the activations that we care about.}
}
$$


##### aside
- now, we can introduce multuple feature maps, where each looks for its own feature in the original image!![[Screenshot 2023-04-25 at 2.48.31 AM.png]]
	- each feature map is like its own layer with its own set of weights and a bias, and looks for a different feature of the original input image
	- this grouping of feature maps is a ***convolutional layer***
- also, convolutional neural networks are beneficial for the fact that we have to deal with much fewer parameters; each feature map only needs $n \times n$ weights, plus 1 bias

### pooling layers
- come after the previously mentioned convolutional layers, and simplify their output
- takes each feature map and "summarizes", or "condenses" it

one pooling technique is ***max-pooling***, where a 2x2 grouping of activations is represented by the largest activation in the region
- naturally, this needs to be applied to every single feature map, to create as many pooled layers as we had feature maps to begin with
~~~
in a way, max pooling enables us to figure out if a certain feature is present in the image, and only keep track of its rough location relative to other features, not exact
~~~

another pooling technique is ***L2 pooling***