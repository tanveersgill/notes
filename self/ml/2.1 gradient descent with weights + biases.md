~~~
the previous note i wrote about the idea behind gradient descent but used an example with a vector 'v' that was a loose representation of the parameters of the cost function.

here's how you actually do it, with the parameters being the weights and biases
~~~
- firstly, we should replace 'v' with w and b
- then, to figure out the 'new position' that will approach the minimum:
  $$
\displaylines{
w_{k}\ '= w_{k}-\eta \frac{\partial C}{\partial w_{k}}
\\and\\
b_{l}\ '=b_{l} -\eta \frac{\partial C}{\partial b_{l}}
\\where\\ w_{k}\ and\ b_{l}\ are\ the\ individual\ weights\ and\ biases
}
$$
