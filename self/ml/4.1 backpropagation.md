```
spamming chain rule to get values that we want, such as the partial derivative of cost in terms of weight and bias
```
remember that each neuron has a 'z' value which it passes into the sigmoid function to produce an output.
- we could figure out the impact that changing the $z$ value of the $j^{th}$ neuron in the $l^{th}$ layer has on the cost, $\partial C/\partial z^{l}_{j}$ 
- ^ that partial derivative right there is the "error" of a neuron
	- because it shows us how large an impact the $z$ value of the neuron has on the cost $C$ 
- knowing how the cost changes with changes to the z value of each neuron is helpful; if we find that $\partial C/\partial z^{l}_{j}$ is large (regardless of sign), it indicates that we should add a $\Delta z^{l}_{j}$ that will counteract the impact that $z^{l}_{j}$ has.


### 4 equations of backpropagation
- that error just discussed is $\delta^{l}_{j} = \frac{\partial C}{\partial z^{l}_{j}}$ 
- backpropagation essentially lets us figure out a vector of errors for each layer, and then connect the error values back to $\partial C/\partial w$ and $\partial C/\partial b$, which we are looking for

***1. error in the output layer***
- we know the error of a single neuron is:
  $$\delta^{L}_{j} = \frac{\partial C}{\partial z^{L}_{j}}$$
- applying chain rule:
$$\delta^{L}_{j} = \frac{\partial C}{\partial a^{L}_{j}}\times\frac{\partial a^{L}_{j}}{\partial z^{L}_{j}}$$
- that second term is the derivative of the sigmoid function:$$\delta^{L}_{j} = \frac{\partial C}{\partial a^{L}_{j}}\times\sigma'(z^{L}_{j})$$
- the first term is a simple derivative that can be computed easily
- with this, we know the error of any neuron in the output layer!
- to get a vector of errors for the last layer:
$$
\delta^{L}=\nabla_{a}C\times\sigma'(z^{L})
$$
***2. error of any neuron***
- the idea is that, if we know the errors of one layer, we can calculate the errors of the previous layer
- it just becomes a problem of spamming chain rule. i'm too lazy right now to write out the derivation, but here is the formula:
$$
\delta^{l}=(W^{l+1})^{T}\cdot(\delta^{l+1})\otimes \sigma'(z^{l})
$$
***3. derivative of cost w.r.t. any bias***
- equal to the error!!!
$$\frac{\partial C}{\partial b^{l}}=\delta^{l}$$
- [derived this on paper, makes sense]
***4. derivative of cost w.r.t. any weight***
- 