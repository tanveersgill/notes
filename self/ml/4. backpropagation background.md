`a fast algorithm to compute the gradient of the cost function`
- more specifically, computing $\partial C/\partial w$ or $\partial C/\partial b$, the partial derivatives of a cost function C with respect to any weight w or bias b
	- this expression tells us how changing weights and biases changes the cost
		- just what we'd expect from a partial derivative

### some background
- we'll still represent weights as a list of matrices, with the $n^{th}$ matrix holding the weights of the $(n+1)^{th}$ layer
	- $w^{n}_{jk}$ denotes, in the $n^{th}$ layer, the weight connecting the $j^{th}$ neuron of the $n^{th}$ layer to the $k^{th}$ neuron of the $(n-1)^{th}$ layer
- biases for a layer are stored in a vector $b^{n}$, and the bias for the $j^{th}$ node is given $b^{n}_{j}$
- `activation` will refer to the output of a single neuron, and will be denoted $a^{n}_{j}$ ; `n` is layer number, `j` is neuron number
	- it is computed as:$$
	  a^{n}_{j} = \sigma(\sum_{k}w^{n}_{jk}a^{n-1}_{k} + b^{n}_{j})
	  $$
	  - or, if we want to compute the activations of the entire layer in one go, we can vectorize this:$$
	  a^{n}=\sigma(w^{n}a^{n-1} + b^{n})$$
	  - just as in the previous note

### cost function assumptions
1. the cost function $C$ is actually an average of cost functions $C_{x}$ over $n$ training examples, for individual training examples $x$:
$$
C=\frac{1}{n}\sum_{x}C_{x}
$$
- and the form of each $C_{x}$ is a cost function (MSE, by the way) as follows:
$$
\frac{1}{2}||y(x)-a^{L}(x)||^{2}
$$
- $y(x) \to expected\ output\ for\ input\ x$
- $a^{L}(x) \to activation\ vector\ at\ last\ layer\ L\ for\ input\ x$
	- in other words, the *actual* output of the network given $x$
2. the cost function $C$ can be written as a function of the neural network's outputs $C(a^{L})$ 

### hadamard product ($\odot$)
- a linear algebra operation on matrices (or vectors) of equal size that just takes the element-wise product
	- each corresponding element is multiplied


