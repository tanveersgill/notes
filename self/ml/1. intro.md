### the perceptron
~~~
works with binary inputs/outputs
~~~
- basic node that takes in *binary* (0 or 1) inputs, and uses a weighted sum of some weights and the inputs to determine if it should output a 1 or 0
$$
\displaylines{
	\vec{w}\ \cdot\ \vec{x} \ compared\ to\ [threshold]
	\\,where\ \vec{x}\ is\ inputs\ and\ \vec{w}\ is\ weights
}
$$
- this dot product IS the weighted sum.
- some threshold value is used to determine if a 1 or 0 should be output.

***bias***
- the threshold, just moved to the other side of the dot product equation
- it is negative for thresholds that were positive:
$$
\displaylines{
output = \begin{cases}
	0,\ if\ \vec{x}\ \cdot\ \vec{w} \ge threshold
	\\
	1, if ...
\end{cases}
\\
becomes
\\
output = \begin{cases}
	0,\ if\ \vec{x}\ \cdot\ \vec{w} - threshold\ge 0
	\\
	1, if ...
\end{cases}
\\ becomes \\\\
output = \begin{cases}
	0,\ if\ (\vec{x}\ \cdot\ \vec{w} + b)\ge 0
	\\
	1, if ...
\end{cases}
\\
and\ so,
\\
b=-threshold
}
$$
> perceptrons can be used to implement digital logic, such as NAND, AND, OR gates

### sigmoid neurons (learning algorithms)
~~~
|-> ideally, we want to be able to slightly change a weight/bias, and have this result in a slight change in the output
	|-> this "learning" will not work with perceptrons, because they have a binary output, and we could potentially flip a zero to a one somewhere, which in turn might completely change the behaviour of the entire network

enter: sigmoid neurons
~~~
- *sigmoid neurons* implement the *sigmoid function*, which yields non-binary outputs
	- meaning that small weight/bias changes result in small output changes

***the function:***
$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$
- if you graph it, it kind of looks like inverse tan, but its outputs go from 0 to 1, not -pi/2 to pi/2
- as such, the neuron only acts like a perceptron for very large and small z values
	- otherwise, it has modest changes

***back on topic:***
- these neurons take in *any* value between 0 and 1, and do the same for their output
- the *weighted sum* is passed into the sigmoid function, and used to compute an output:
~~~
x1 --
      \
x2 ------- Ïƒ --> output
      /
x3 -- 
~~~
$$
\displaylines{
first,\ compute\ z=(\vec{w}\cdot\vec{x}\ +\ b)
\\
then,\ pass\ to\ sigmoid\ function:
\sigma(z)
\\
to\ yield\ an\ output\ between\ 0\to1
}
$$

***figuring out how changing weights/biases will impact the output*** ***(total differential)***
~~~
[not to be confused with a total derivative!]
in multivariable calculus, the 'total differential' gives us a way to calculate the total change in a function of multiple variables with changes in all of its variables at once:

|-> it is an extension of the multivariable chain rule

so, a normal partial derivative tells us how much the function will change with a tiny change in one variable

on the other hand, taking a total differential tells us how the entire function will change given changes in the variables it depends upon
~~~
$$
\displaylines{
for\ z=f(x,y, ...),
\\
dz = \frac{\partial z}{\partial x}dx + \frac{\partial z}{\partial y}dy\ +\ ...
}
$$
- using the total differential, we can compute the ğš«output (change in output) that would result from changing certain weights and biases
	- because output is basically a function of weight and bias
$$
\Delta output \approx \sum_{i}\frac{\partial z}{\partial w_{i}}\Delta w_{i} + \frac{\partial z}{\partial b}\Delta b\
$$
- and now we know how the output value would change with a change in weights/biases!
- notice ğš«output is a linear function; that is helpful  